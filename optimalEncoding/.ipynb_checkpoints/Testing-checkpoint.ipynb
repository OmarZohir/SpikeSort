{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#equivalent of twoDWhite.m file\n",
    "\n",
    "Nneuron=20; # size of the population\n",
    "Nx=2;       # dimesnion of the input\n",
    "\n",
    "lamda=50;    #membrane leak, renamed from lamda to lamda, to avoid confusion with lamda expressions\n",
    "dt=0.001;     #time step\n",
    "\n",
    "epsr=0.001;  # earning rate of the recurrent connections\n",
    "epsf=0.0001; ## learning rate of the feedforward connections FF\n",
    "\n",
    "alpha=0.18; # scaling of the Feefforward weights\n",
    "beta=1/0.9;  #scaling of the recurrent weights\n",
    "mu=0.02/0.9; #quadratic cost\n",
    "\n",
    "\n",
    "##Initial connectivity\n",
    "\n",
    "Fi=0.5*np.random.randn(Nx,Nneuron); #the inital feedforward weights are chosen randomely\n",
    "Fi = 1*np.divide(Fi,(np.sqrt(np.ones((Nx,1))*(np.sum(np.multiply(Fi,Fi))))))\n",
    "#Fi=1*(Fi./(np.sqrt(np.ones(Nx,1)*(np.sum(Fi.^2)))));  #the FF weights are normalized\n",
    "Ci=-0.2*(np.random.rand(Nneuron,Nneuron))-0.5*np.eye(Nneuron); #the initial recurrent conectivity is very weak except for the autapses\n",
    "\n",
    "Thresh=0.5; #vector of thresholds of the neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "###   This function  performs the learning of the\n",
    "####  recurrent and feedforward connectivity matrices.\n",
    "####\n",
    "####\n",
    "####  it takes as an argument the time step ,dt, the membrane leak, lamda, \n",
    "####  the learning rate of the feedforward and the recurrent\n",
    "####  conections epsf and epsr, the scaling parameters alpha and beta of\n",
    "####  the weights, mu the quadratic cost, the number of neurons on the\n",
    "####  population Nneuron, the dimension of the input, the threshold of\n",
    "####  the neurons  an the initial feedforward and recuurrent connectivity F\n",
    "####  and C.\n",
    "####\n",
    "####   The output of this function are arrays, Fs abd Cs, containning the\n",
    "####   connectivity matrices sampled at exponential time instances Fs and\n",
    "####   Cs , The Final connectivity matrices F and C. It also gives the\n",
    "####   Optimal decoders for each couple of recurrent and feedforward\n",
    "####   connectivities registered in Fs and Cs. The output ErrorC contains\n",
    "####   the distance between the current and optimal recurrent connectivity\n",
    "####   stored in Cs. \n",
    "####\n",
    "####   It also produces two figures. The first one it repsents the\n",
    "####   connectivities before and after learning and the second figure\n",
    "####   represents the performance of the network through learning. \n",
    "####\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "\n",
    "##\n",
    "##################################################################################\n",
    "######################   Learning the optinal connectivities  ####################\n",
    "##################################################################################\n",
    "##################################################################################\n",
    "\n",
    "Nit=14000;                   #number of iteration\n",
    "Ntime=1000;                  #size of an input sequence\n",
    "TotTime=Nit*Ntime           #total time of Learning\n",
    "T= int(np.floor(np.log(TotTime)/np.log(2))); #Computing the size of the matrix where the weights are stocked on times defined on an exponential scale \n",
    "Cs=np.zeros((T,Nneuron, Nneuron)); #the array that contains the different instances of reccurent connectivty through learning\n",
    "Fs=np.zeros((T,Nx, Nneuron));      #the array that contains the different instances of feedforward connectivty through learning\n",
    "V=np.zeros((Nneuron,)); #voltage vector of the population\n",
    "O=0;  #variable indicating the eventual  firing of a spike\n",
    "k=1;  #index of the neuron that fired\n",
    "rO=np.zeros((Nneuron,)); #vector of filtered spike train\n",
    "\n",
    "\n",
    "x=np.zeros((Nx,));   #filtered input\n",
    "Input=np.zeros((Nx,Ntime)); #raw input to the network\n",
    "Id=np.eye(Nneuron); #identity matrix\n",
    "\n",
    "A=2000; #Amplitude of the input\n",
    "sigma=np.absolute(30); #std of the smoothing kernel\n",
    "Gaussdist = np.arange(1,1001)-500;  #some random Gaussian distribution, starts from -499 ends at 500 with almost zero mean\n",
    "w=(1/(sigma*np.sqrt(2* np.pi)))* np.exp(-(Gaussdist*Gaussdist)/(2*sigma*sigma));#gaussian smoothing kernel used to smooth the input\n",
    "w=w/np.sum(w); # normalization oof the kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 % of the learning  completed\n",
      "\n",
      "1 % of the learning  completed\n",
      "\n",
      "2 % of the learning  completed\n",
      "\n",
      "3 % of the learning  completed\n",
      "\n",
      "4 % of the learning  completed\n",
      "\n",
      "5 % of the learning  completed\n",
      "\n",
      "6 % of the learning  completed\n",
      "\n",
      "7 % of the learning  completed\n",
      "\n",
      "8 % of the learning  completed\n",
      "\n",
      "9 % of the learning  completed\n",
      "\n",
      "10 % of the learning  completed\n",
      "\n",
      "11 % of the learning  completed\n",
      "\n",
      "12 % of the learning  completed\n",
      "\n",
      "13 % of the learning  completed\n",
      "\n",
      "14 % of the learning  completed\n",
      "\n",
      "15 % of the learning  completed\n",
      "\n",
      "16 % of the learning  completed\n",
      "\n",
      "17 % of the learning  completed\n",
      "\n",
      "18 % of the learning  completed\n",
      "\n",
      "19 % of the learning  completed\n",
      "\n",
      "20 % of the learning  completed\n",
      "\n",
      "21 % of the learning  completed\n",
      "\n",
      "22 % of the learning  completed\n",
      "\n",
      "23 % of the learning  completed\n",
      "\n",
      "24 % of the learning  completed\n",
      "\n",
      "25 % of the learning  completed\n",
      "\n",
      "26 % of the learning  completed\n",
      "\n",
      "27 % of the learning  completed\n",
      "\n",
      "28 % of the learning  completed\n",
      "\n",
      "29 % of the learning  completed\n",
      "\n",
      "30 % of the learning  completed\n",
      "\n",
      "31 % of the learning  completed\n",
      "\n",
      "32 % of the learning  completed\n",
      "\n",
      "33 % of the learning  completed\n",
      "\n",
      "34 % of the learning  completed\n",
      "\n",
      "35 % of the learning  completed\n",
      "\n",
      "36 % of the learning  completed\n",
      "\n",
      "37 % of the learning  completed\n",
      "\n",
      "38 % of the learning  completed\n",
      "\n",
      "39 % of the learning  completed\n",
      "\n",
      "40 % of the learning  completed\n",
      "\n",
      "41 % of the learning  completed\n",
      "\n",
      "42 % of the learning  completed\n",
      "\n",
      "43 % of the learning  completed\n",
      "\n",
      "44 % of the learning  completed\n",
      "\n",
      "45 % of the learning  completed\n",
      "\n",
      "46 % of the learning  completed\n",
      "\n",
      "47 % of the learning  completed\n",
      "\n",
      "48 % of the learning  completed\n",
      "\n",
      "49 % of the learning  completed\n",
      "\n",
      "50 % of the learning  completed\n",
      "\n",
      "51 % of the learning  completed\n",
      "\n",
      "52 % of the learning  completed\n",
      "\n",
      "53 % of the learning  completed\n",
      "\n",
      "54 % of the learning  completed\n",
      "\n",
      "55 % of the learning  completed\n",
      "\n",
      "56 % of the learning  completed\n",
      "\n",
      "57 % of the learning  completed\n",
      "\n",
      "58 % of the learning  completed\n",
      "\n",
      "59 % of the learning  completed\n",
      "\n",
      "60 % of the learning  completed\n",
      "\n",
      "61 % of the learning  completed\n",
      "\n",
      "62 % of the learning  completed\n",
      "\n",
      "63 % of the learning  completed\n",
      "\n",
      "64 % of the learning  completed\n",
      "\n",
      "65 % of the learning  completed\n",
      "\n",
      "66 % of the learning  completed\n",
      "\n",
      "67 % of the learning  completed\n",
      "\n",
      "68 % of the learning  completed\n",
      "\n",
      "69 % of the learning  completed\n",
      "\n",
      "70 % of the learning  completed\n",
      "\n",
      "71 % of the learning  completed\n",
      "\n",
      "72 % of the learning  completed\n",
      "\n",
      "73 % of the learning  completed\n",
      "\n",
      "74 % of the learning  completed\n",
      "\n",
      "75 % of the learning  completed\n",
      "\n",
      "76 % of the learning  completed\n",
      "\n",
      "77 % of the learning  completed\n",
      "\n",
      "78 % of the learning  completed\n",
      "\n",
      "79 % of the learning  completed\n",
      "\n",
      "80 % of the learning  completed\n",
      "\n",
      "81 % of the learning  completed\n",
      "\n",
      "82 % of the learning  completed\n",
      "\n",
      "83 % of the learning  completed\n",
      "\n",
      "84 % of the learning  completed\n",
      "\n",
      "85 % of the learning  completed\n",
      "\n",
      "86 % of the learning  completed\n",
      "\n",
      "87 % of the learning  completed\n",
      "\n",
      "88 % of the learning  completed\n",
      "\n",
      "89 % of the learning  completed\n",
      "\n",
      "90 % of the learning  completed\n",
      "\n",
      "91 % of the learning  completed\n",
      "\n",
      "92 % of the learning  completed\n",
      "\n",
      "93 % of the learning  completed\n",
      "\n",
      "94 % of the learning  completed\n",
      "\n",
      "95 % of the learning  completed\n",
      "\n",
      "96 % of the learning  completed\n",
      "\n",
      "97 % of the learning  completed\n",
      "\n",
      "98 % of the learning  completed\n",
      "\n",
      "99 % of the learning  completed\n",
      "\n",
      "learning  completed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "j=1; # index of the (2^j)-time step (exponential times)\n",
    "l=1;\n",
    "F = Fi;\n",
    "C= Ci;\n",
    "print(f\"{0} % of the learning  completed\\n\");\n",
    " \n",
    "for i in range (2,TotTime):\n",
    "    if ((i/TotTime)>(l/100)):\n",
    "        print(f\"{l} % of the learning  completed\\n\");\n",
    "        l=l+1;\n",
    "    \n",
    "    \n",
    "    if (np.mod(i,np.power(2,j))==0): #registering ther weights on an exponential time scale 2^j\n",
    "        Cs[j-1,:,:]=C;   #registering the recurrent weights\n",
    "        Fs[j-1,:,:]=F;   #registering the Feedfoward weights\n",
    "        j=j+1;\n",
    "    \n",
    "    if (np.mod(i-2,Ntime)==0): #Generating a new iput sequence every Ntime time steps \n",
    "        meandim = np.zeros((1,Nx)).ndim;\n",
    "        meanList = meandim*[0];\n",
    "        cov = np.eye(Nx);\n",
    "        #Mean has to be passed as a list not an array\n",
    "        Input  = np.transpose(np.random.multivariate_normal(meanList,cov,Ntime)); #generating a new sequence of input which a gaussion vector\n",
    "        for d in range (0,Nx-1):\n",
    "            Input[d,:] = A*np.convolve(Input[d,:],w,'same'); #smoothing the previously generated white noise with the gaussian window w\n",
    "             \n",
    "    \n",
    "    V=(1-lamda*dt)*np.array(V) + dt * np.matmul(np.transpose(F),Input[:,np.mod(i,Ntime)])+ O*C[:,k]+0.001*np.random.randn(Nneuron,); #the membrane potential is a leaky integration of the feedforward input and the spikes\n",
    "    x=(1-lamda*dt)*x+dt*Input[:,np.mod(i,Ntime)]; #filtered input\n",
    "         \n",
    "    tempArr = V - Thresh-0.01*np.random.randn(Nneuron,)-0;\n",
    "    m = np.max(tempArr); #finding the neuron with largest membrane potential\n",
    "    k = np.argmax(tempArr);\n",
    "    \n",
    "    \n",
    "    if (m>=0): #if its membrane potential exceeds the threshold the neuron k spikes  \n",
    "        O=1; # the spike ariable is turned to one\n",
    "        F[:,k]=F[:,k]+epsf*(alpha*x-F[:,k]); #updating the feedforward weights\n",
    "        C[:,k]=C[:,k] -(epsr)*(beta*(V+ mu*rO)+C[:,k]+mu*Id[:,k]);#updating the recurrent weights\n",
    "        rO[k,]=rO[k,]+1; #updating the filtered spike train\n",
    "    else:\n",
    "        O=0;\n",
    "    \n",
    "    \n",
    "    rO=(1-lamda*dt)*rO; #filtering the spikes\n",
    "    \n",
    "print(\"learning  completed!\\n\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing optimal decoders\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-48-1a4f0c2b6cbc>:35: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  Dec = np.transpose(np.linalg.lstsq(np.transpose(rOL),np.transpose(xL))[0]);\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Decoder done\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "##################################################################################\n",
    "######################   Computing Optimal Decoders  #############################\n",
    "##################################################################################\n",
    "##################################################################################\n",
    "#####\n",
    "##### After having learned the connectivities F and C we compute the\n",
    "##### optimal decoding weights for each instance of the network defined by\n",
    "##### the pairs of the FF and recurr connectivitiy matrices stocked\n",
    "##### previously in arrays  Fs and Cs. This will allow us to compute the\n",
    "##### decoding error over learning.\n",
    "#####\n",
    "##################################################################################\n",
    "##################################################################################\n",
    "##################################################################################\n",
    "##################################################################################\n",
    "print(\"Computing optimal decoders\\n\\n\");\n",
    "TimeL=50000; # size of the sequence  of the input that will be fed to neuron\n",
    "xL=np.zeros((Nx,TimeL)); # the target output/input\n",
    "Decs=np.zeros((T,Nx,Nneuron));# array where the decoding weights for each instance of the network will be stocked\n",
    "InputL=0.3*A*np.transpose((np.random.multivariate_normal(np.zeros((Nx,)),np.eye(Nx),TimeL))); #generating a new input sequence\n",
    "\n",
    "for k in range (0,Nx-1):\n",
    "    InputL[k,:]=np.convolve(InputL[k,:],w,'same'); #smoothing the input as before\n",
    "\n",
    "\n",
    "for t in range (1,TimeL-1):\n",
    "    xL[:,t]= (1-lamda*dt)*xL[:,t-1]+ dt*InputL[:,t-1]; #compute the target output by a leaky integration of the input  \n",
    "\n",
    "\n",
    "\n",
    "for i in range (0,T-1):\n",
    "    rOL, _ , _ = runnet(dt, lamda, np.squeeze(Fs[i,:,:]), InputL, np.squeeze(Cs[i,:,:]),Nneuron,TimeL, Thresh); # running the network with the previously generated input for the i-th instanc eof the network\n",
    "    #Dec=np.transpose(np.matmul(np.transpose(rOL),np.linalg.inv(np.transpose(xL)))); # computing the optimal decoder that solves xL=Dec*rOL,  Dec=(rOL'\\xL')';\n",
    "    Dec = np.transpose(np.linalg.lstsq(np.transpose(rOL),np.transpose(xL))[0]);\n",
    "    #Dec = np.linalg.lstsq(rOL,xL);\n",
    "    Decs[i,:,:]=Dec; # stocking the decoder in Decs\n",
    "    \n",
    "print(\"Optimal Decoder done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#################################################################################\n",
    "###########  Computing Decoding Error, rates through Learning ###################\n",
    "#################################################################################\n",
    "#################################################################################\n",
    "#################################################################################\n",
    "#####\n",
    "##### In this part we run the different instances of the network using a\n",
    "##### new test input and we measure the evolution of the dedocding error\n",
    "##### through learning using the decoders that we computed preciously. We also\n",
    "##### measure the evolution of the mean firing rate anf the variance of the\n",
    "##### membrane potential.\n",
    "#####\n",
    "#################################################################################\n",
    "#################################################################################\n",
    "#################################################################################\n",
    "#################################################################################\n",
    "\n",
    "print('Computing decoding errors and rates over learning\\n')\n",
    "TimeT=10000; # size of the test input\n",
    "MeanPrate=zeros((1,T));#array of the mean rates over learning\n",
    "Error=zeros((1,T));#array of the decoding error over learning\n",
    "MembraneVar=zeros((1,T));#mean membrane potential variance over learning\n",
    "xT=zeros((Nx,TimeT));#target ouput\n",
    "\n",
    "\n",
    "\n",
    "Trials=10; #number of trials\n",
    "\n",
    "for r in range (1,Trials): #for each trial\n",
    "    InputT =A*(mvnrnd(zeros(1,Nx),eye(Nx),TimeT))'; # we genrate a new input\n",
    "    \n",
    "    for k=1:Nx\n",
    "        InputT(k,:)=conv(InputT(k,:),w,'same'); # we wmooth it\n",
    "    end\n",
    "    \n",
    "    for t=2:TimeT      \n",
    "        xT(:,t)= (1-lambda*dt)*xT(:,t-1)+ dt*InputT(:,t-1); # ans we comput the target output by leaky inegration of the input       \n",
    "    end    \n",
    "    \n",
    "    for i=1:T #for each instance of the network\n",
    "        [rOT, OT, VT] = runnet(dt, lambda, squeeze(Fs(i,:,:)) ,InputT, squeeze(Cs(i,:,:)),Nneuron,TimeT, Thresh);#we run the network with current input InputL\n",
    "        \n",
    "        xestc=squeeze(Decs(i,:,:))*rOT; #we deocode the ouptut using the optinal decoders previously computed\n",
    "        Error(1,i)=Error(1,i)+sum(var(xT-xestc,0,2))/(sum(var(xT,0,2))*Trials);#we comput the variance of the error normalized by the variance of the target\n",
    "        MeanPrate(1,i)=MeanPrate(1,i)+sum(sum(OT))/(TimeT*dt*Nneuron*Trials);#we comput the average firing rate per neuron\n",
    "        MembraneVar(1,i)=MembraneVar(1,i)+sum(var(VT,0,2))/(Nneuron*Trials);# we compute the average membrane potential variance per neuron     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runnet(dt, lamda, F ,Input, C, Nneuron, Ntime, Thresh):\n",
    "\n",
    "##############################################################################\n",
    "##############################################################################\n",
    "##############################################################################\n",
    "##############################################################################\n",
    "####\n",
    "#### This function runs the network without learning. It take as an\n",
    "#### argument the time step dt, the leak of the membrane potential lamda,\n",
    "#### the Input of the network, the recurrent connectivity matrix C, the feedforward\n",
    "#### connectivity matrix F, the number of neurons Nneuron, the length of\n",
    "#### the Input Ntime, and the Threhsold. It returns the spike trains O\n",
    "#### the filterd spike trains rO, and the membrane potentials V.\n",
    "####\n",
    "##############################################################################\n",
    "##############################################################################\n",
    "##############################################################################\n",
    "##############################################################################\n",
    "\n",
    "    rO=np.zeros((Nneuron,Ntime));#filtered spike trains\n",
    "    O=np.zeros((Nneuron,Ntime)); #spike trains array\n",
    "    V=np.zeros((Nneuron,Ntime)); #mamebrane poterial array\n",
    "\n",
    "    for t in range (1,Ntime-1):\n",
    "\n",
    "        V[:,t]=(1-lamda*dt)*V[:,t-1]+dt * np.matmul(np.transpose(F),Input[:,t-1]) + np.matmul(C,O[:,t-1])+0.001*np.random.randn(Nneuron,);#the membrane potential is a leaky integration of the feedforward input and the spikes\n",
    "\n",
    "        tempArr = V[:,t] - Thresh-0.01* np.random.randn(Nneuron,);\n",
    "        m= np.max(tempArr);#finding the neuron with largest membrane potential\n",
    "        k = np.argmax(tempArr);\n",
    "\n",
    "        if (m>=0):  #if its membrane potential exceeds the threshold the neuron k spikes  \n",
    "            O[k,t]=1; # the spike ariable is turned to one\n",
    "\n",
    "        rO[:,t]=(1-lamda*dt)*rO[:,t-1]+1*O[:,t]; #filtering the spikes\n",
    "\n",
    "    return rO, O, V; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meandim = np.zeros((1,Nx)).ndim;\n",
    "meanList = meandim*[0];\n",
    "print([0,0])\n",
    "print(meanList)\n",
    "print(np.eye(Nx))\n",
    "print(np.zeros((1,Nx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape((1-lamda*dt)*V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(0.001*np.random.randn(Nneuron,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(O*C[:,k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(np.transpose(F))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(O*np.array(C[:,k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.amax(V - Thresh-0.01*np.random.randn(Nneuron,1)-0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " np.shape( O*Ci[:,k] + 0.001*np.random.randn(Nneuron,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(dt * np.matmul(np.transpose(Fi),Input[:,np.mod(1,Ntime)+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " np.shape(O*Ci[:,k] +0.001*np.random.randn(Nneuron,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1-lamda*dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1-lamda*dt)*V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fi=0.5*np.random.randn(Nx,Nneuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fi = 1*np.divide(Fi,(np.sqrt(np.ones((Nx,1))*(np.sum(np.multiply(Fi,Fi))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.sum(np.multiply(Fi,Fi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.ones((Nx,1))*(np.sum(np.multiply(Fi,Fi))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mod(i-2,Ntime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Inputx, Inputy  = np.transpose(np.random.multivariate_normal(meanList,cov,Ntime)); #generating a new sequence of input which"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Inputx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Inputy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(Input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V - Thresh-0.01*np.random.randn(Nneuron,)-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(Cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mod(i,2^j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2^24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((1,Nx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((Nx,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(np.transpose(np.zeros((Nx,))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(Input[:,500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-ac9c828b15ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.95\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m499\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-lamda*dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-3e82b1464119>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mO\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(Dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
